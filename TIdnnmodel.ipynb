{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "from keras.preprocessing import image\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "vgg16 = VGG16(include_top=False, weights = 'imagenet', input_shape=(224, 224, 3))\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "def build_transfer_model(vgg16):\n",
    "    model =  Sequential(vgg16.layers)\n",
    "     \n",
    "    for layer in model.layers[:15]:\n",
    "           layer.trainable = True\n",
    "            \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(7,activation = 'softmax')) \n",
    "        \n",
    "    return model\n",
    "        \n",
    "model = build_transfer_model(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer=Adam(lr=0.01),  \n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "idg_train = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=60,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.3,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    ")\n",
    "\n",
    "idg_validation = ImageDataGenerator(\n",
    "    rescale=1/255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes=['0a3','0b2','0c1','a0','a1','a2','a3']\n",
    "\n",
    "\n",
    "img_itr_train= idg_train.flow_from_directory(\n",
    "    'E:\\train',\n",
    "    #training data folder\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    classes=classes,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "img_itr_validation = idg_validation.flow_from_directory(\n",
    "    'E:\\validation',\n",
    "    #validation data folder\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    classes=classes,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "model_dir = os.path.join(\n",
    "    'models',\n",
    "    datetime.now().strftime('%y%m%d_%H%M')\n",
    ")\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "            \n",
    "dir_weights = os.path.join(model_dir, 'weights')\n",
    "os.makedirs(dir_weights, exist_ok=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "model_json = os.path.join(model_dir, 'model.json')\n",
    "with open(model_json, 'w') as f:\n",
    "    json.dump(model.to_json(),f)\n",
    "        \n",
    "model_classes = os.path.join(model_dir, 'classes.pkl')\n",
    "with open(model_classes, 'wb') as f:\n",
    "    pickle.dump(img_itr_train.class_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 32\n",
    "steps_per_epoch = math.ceil(\n",
    "    img_itr_train.samples/batch_size\n",
    ")\n",
    "\n",
    "validation_steps = math.ceil(\n",
    "    img_itr_validation.samples/batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "cp_filepath = os.path.join(dir_weights, 'ep_{epoch:02d}_ls_{loss:.1f}.h5')\n",
    "cp = ModelCheckpoint(\n",
    "                    cp_filepath,\n",
    "                    monitor='val_acc',\n",
    "                    verbose=0,\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=False,\n",
    "                    mode='max',\n",
    "                    period =10\n",
    "                    )\n",
    "\n",
    "csv_filepath = os.path.join(model_dir, 'loss.csv')\n",
    "csv = CSVLogger(csv_filepath, append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "\n",
    "history = model.fit_generator(\n",
    "    img_itr_train,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=n_epoch,\n",
    "    validation_data=img_itr_validation,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks = [cp, csv]\n",
    "    \n",
    ")\n",
    "\n",
    "model.save(os.path.join(model_dir,'TI_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#predict by confusion matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "import keras\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, list_pictures, load_img\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "path='E:\\ '\n",
    "# test data path\n",
    "folders=glob.glob(path+'/**', recursive=False)\n",
    "category_number=len(folders)\n",
    "categoryname=[]\n",
    "\n",
    "model=load_model('TI_model.h5')\n",
    "\n",
    "X =[]\n",
    "Y = []\n",
    "Fnames=[]\n",
    "size=224 #image size\n",
    "\n",
    "for category_num, folder in enumerate(folders):\n",
    "    categoryname.append(os.path.basename(folder))\n",
    "    print(category_num)\n",
    "    \n",
    "    files=glob.glob(folder+\"/*png\")\n",
    "    for filepath in files:\n",
    "        \n",
    "        img=Image.open(filepath)\n",
    "        img=img.convert(\"RGB\")\n",
    "        img=img.resize((size,size))\n",
    "        img=np.asarray(img)\n",
    "        X.append(img)\n",
    "        Y.append(category_num) \n",
    "        Fnames.append(os.path.basename(filepath))\n",
    "    \n",
    "   \n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n",
    "\n",
    "X = X.astype('float32')/255\n",
    "Y = Y.astype('int32')\n",
    "\n",
    "Y = np_utils.to_categorical(Y, category_number) \n",
    "\n",
    "#predict\n",
    "print('predicting')\n",
    "probs=model.predict(X, verbose=1)\n",
    "label=probs.argmax(axis=1)\n",
    "\n",
    "\n",
    "#confusion matrix\n",
    "predict_classes = model.predict_classes(X, batch_size=32)\n",
    "true_classes = np.argmax(Y,1)\n",
    "print(confusion_matrix(true_classes, predict_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
